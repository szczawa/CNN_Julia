{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definicja struktur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end\n",
    "\n",
    "struct Constant{T} <: GraphNode\n",
    "    output :: T\n",
    "end\n",
    "\n",
    "mutable struct Variable <: GraphNode\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    Variable(output; name=\"?\") = new(output, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct ScalarOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    ScalarOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct BroadcastedOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    BroadcastedOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wizualizacja grafu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show (generic function with 282 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Base: show, summary\n",
    "show(io::IO, x::ScalarOperator{F}) where {F} = print(io, \"op \", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::BroadcastedOperator{F}) where {F} = print(io, \"op.\", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::Constant) = print(io, \"const \", x.output)\n",
    "show(io::IO, x::Variable) = begin\n",
    "    print(io, \"var \", x.name);\n",
    "    print(io, \"\\n ┣━ ^ \"); summary(io, x.output)\n",
    "    print(io, \"\\n ┗━ ∇ \");  summary(io, x.gradient)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Budowa grafu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topological_sort (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function visit(node::GraphNode, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "    \n",
    "function visit(node::Operator, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function topological_sort(head::GraphNode)\n",
    "    visited = Set()\n",
    "    order = Vector()\n",
    "    visit(head, visited, order)\n",
    "    return order\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset!(node::Constant) = nothing\n",
    "reset!(node::Variable) = node.gradient = nothing\n",
    "reset!(node::Operator) = node.gradient = nothing\n",
    "\n",
    "compute!(node::Constant) = nothing\n",
    "compute!(node::Variable) = nothing\n",
    "compute!(node::Operator) =\n",
    "    node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "\n",
    "function forward!(order::Vector)\n",
    "    for node in order\n",
    "        compute!(node)\n",
    "        reset!(node)\n",
    "    end\n",
    "    return last(order).output\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "update!(node::Constant, gradient) = nothing\n",
    "update!(node::GraphNode, gradient) = if isnothing(node.gradient)\n",
    "    node.gradient = gradient else node.gradient .+= gradient\n",
    "end\n",
    "\n",
    "function backward!(order::Vector; seed=1.0)\n",
    "    result = last(order)\n",
    "    result.gradient = seed\n",
    "    @assert length(result.output) == 1 \"Gradient is defined only for scalar functions\"\n",
    "    for node in reverse(order)\n",
    "        backward!(node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function backward!(node::Constant) end\n",
    "function backward!(node::Variable) end\n",
    "function backward!(node::Operator)\n",
    "    inputs = node.inputs\n",
    "    gradients = backward(node, [input.output for input in inputs]..., node.gradient)\n",
    "    for (input, gradient) in zip(inputs, gradients)\n",
    "        update!(input, gradient)\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementowane operacje skalarne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Base: ^\n",
    "^(x::GraphNode, n::GraphNode) = ScalarOperator(^, x, n)\n",
    "forward(::ScalarOperator{typeof(^)}, x, n) = return x^n\n",
    "backward(::ScalarOperator{typeof(^)}, x, n, g) = tuple(g * n * x ^ (n-1), g * log(abs(x)) * x ^ n)\n",
    "\n",
    "import Base: sin\n",
    "sin(x::GraphNode) = ScalarOperator(sin, x)\n",
    "forward(::ScalarOperator{typeof(sin)}, x) = return sin(x)\n",
    "backward(::ScalarOperator{typeof(sin)}, x, g) = tuple(g * cos(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operacje macierzowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 9 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Base: *\n",
    "import LinearAlgebra: mul!, diagm\n",
    "# x * y (aka matrix multiplication)\n",
    "*(A::GraphNode, x::GraphNode) = BroadcastedOperator(mul!, A, x)\n",
    "forward(::BroadcastedOperator{typeof(mul!)}, A, x) = return A * x\n",
    "backward(::BroadcastedOperator{typeof(mul!)}, A, x, g) = tuple(g * x', A' * g)\n",
    "\n",
    "# x .* y (element-wise multiplication)\n",
    "Base.Broadcast.broadcasted(*, x::GraphNode, y::GraphNode) = BroadcastedOperator(*, x, y)\n",
    "forward(::BroadcastedOperator{typeof(*)}, x, y) = return x .* y\n",
    "backward(node::BroadcastedOperator{typeof(*)}, x, y, g) = let\n",
    "    𝟏 = ones(length(node.output))\n",
    "    Jx = diagm(y .* 𝟏)\n",
    "    Jy = diagm(x .* 𝟏)\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end\n",
    "\n",
    "Base.Broadcast.broadcasted(-, x::GraphNode, y::GraphNode) = BroadcastedOperator(-, x, y)\n",
    "forward(::BroadcastedOperator{typeof(-)}, x, y) = return x .- y\n",
    "backward(::BroadcastedOperator{typeof(-)}, x, y, g) = tuple(g,-g)\n",
    "\n",
    "Base.Broadcast.broadcasted(+, x::GraphNode, y::GraphNode) = BroadcastedOperator(+, x, y)\n",
    "forward(::BroadcastedOperator{typeof(+)}, x, y) = return x .+ y\n",
    "backward(::BroadcastedOperator{typeof(+)}, x, y, g) = tuple(g, g)\n",
    "\n",
    "import Base: sum\n",
    "sum(x::GraphNode) = BroadcastedOperator(sum, x)\n",
    "forward(::BroadcastedOperator{typeof(sum)}, x) = return sum(x)\n",
    "backward(::BroadcastedOperator{typeof(sum)}, x, g) = let\n",
    "    𝟏 = ones(length(x))\n",
    "    J = 𝟏'\n",
    "    tuple(J' * g)\n",
    "end\n",
    "\n",
    "Base.Broadcast.broadcasted(/, x::GraphNode, y::GraphNode) = BroadcastedOperator(/, x, y)\n",
    "forward(::BroadcastedOperator{typeof(/)}, x, y) = return x ./ y\n",
    "backward(node::BroadcastedOperator{typeof(/)}, x, y::Real, g) = let\n",
    "    𝟏 = ones(length(node.output))\n",
    "    Jx = diagm(𝟏 ./ y)\n",
    "    Jy = (-x ./ y .^2)\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end\n",
    "\n",
    "import Base: max\n",
    "Base.Broadcast.broadcasted(max, x::GraphNode, y::GraphNode) = BroadcastedOperator(max, x, y)\n",
    "forward(::BroadcastedOperator{typeof(max)}, x, y) = return max.(x, y)\n",
    "backward(::BroadcastedOperator{typeof(max)}, x, y, g) = let\n",
    "    Jx = diagm(isless.(y, x))\n",
    "    Jy = diagm(isless.(x, y))\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(features = [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; … ;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], targets = [7, 2, 1, 0, 4, 1, 4, 9, 5, 9  …  7, 8, 9, 0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using MLDatasets\n",
    "\n",
    "# load training set\n",
    "train_x, train_y = MNIST.(split=:train)[:]\n",
    "\n",
    "# load test set\n",
    "test_x,  test_y  = MNIST.(split=:test)[:]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dopracowanie formatu danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10000 Matrix{Int64}:\n",
       " 0  0  0  1  0  0  0  0  0  0  1  0  0  …  0  0  0  0  0  1  0  0  0  0  0  0\n",
       " 0  0  1  0  0  1  0  0  0  0  0  0  0     0  0  0  0  0  0  1  0  0  0  0  0\n",
       " 0  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  1  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  0  0  0\n",
       " 0  0  0  0  1  0  1  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  1  0  0\n",
       " 0  0  0  0  0  0  0  0  1  0  0  0  0  …  1  0  0  0  0  0  0  0  0  0  1  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  1  0     0  1  0  0  0  0  0  0  0  0  0  1\n",
       " 1  0  0  0  0  0  0  0  0  0  0  0  0     0  0  1  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  1  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  1  0  1  0  0  1     0  0  0  0  1  0  0  0  0  0  0  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function onehotencode(y::AbstractVector{T}) where T<:Integer\n",
    "    # Znajdujemy unikalne klasy w wektorze `y`\n",
    "    classes = sort(unique(y))\n",
    "    # Liczba klas\n",
    "    num_classes = length(classes)\n",
    "    # Liczba próbek\n",
    "    num_samples = length(y)\n",
    "    # Inicjujemy macierz one-hot encoding `y`\n",
    "    y_onehot = zeros(T, num_classes, num_samples)\n",
    "    # Iterujemy po próbkach i ustawiamy wartości w macierzy one-hot encoding\n",
    "    for i = 1:num_samples\n",
    "        # Znajdujemy indeks klasy odpowiadającej elementowi `y[i]`\n",
    "        index = findfirst(classes .== y[i])\n",
    "        # Ustawiamy 1 w odpowiednim miejscu w macierzy\n",
    "        for j = 1:num_classes\n",
    "            if j == index\n",
    "                y_onehot[j, i] = 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    # Zwracamy macierz one-hot encoding `y`\n",
    "    return y_onehot\n",
    "end\n",
    "\n",
    "train_y_onehot = onehotencode(train_y)\n",
    "test_y_onehot = onehotencode(test_y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konwolucja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 10 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv(x::GraphNode, f::GraphNode) = BroadcastedOperator(conv, x, f)\n",
    "forward(::BroadcastedOperator{typeof(conv)}, x, f) = let \n",
    "# rozmiary danych wejściowych\n",
    "m, n, c = size(x)\n",
    "p, q, ci, k = size(f)\n",
    "\n",
    "# wynikowa macierz konwolucji\n",
    "output = zeros(m-p+1, n-q+1, k)\n",
    "\n",
    "# pętla po każdym filtrowaniu\n",
    "for l = 1:k\n",
    "    # pętla po każdym kanałach\n",
    "    for d = 1:c\n",
    "        # pętla po każdym rzędzie obrazu wejściowego\n",
    "        for i = 1:m-p+1\n",
    "            # pętla po każdej kolumnie obrazu wejściowego\n",
    "            for j = 1:n-q+1\n",
    "                # wartość pojedynczego elementu wynikowego\n",
    "                val = 0.0\n",
    "                # pętla po każdym elemencie w fragmencie obrazu\n",
    "                for s = 1:p\n",
    "                    for t = 1:q\n",
    "                        # iloczyn skalarny\n",
    "                        val += x[i+s-1, j+t-1, d] .* f[s, t, d, l]\n",
    "                    end\n",
    "                end\n",
    "                # przypisanie wartości do macierzy wynikowej\n",
    "                output[i, j, l] += val\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "output\n",
    "end\n",
    "backward(node::BroadcastedOperator{typeof(conv)}, x, f, g) = let\n",
    "    # rozmiary danych wejściowych\n",
    "    m, n, c = size(x)\n",
    "    p, q, ci, k = size(f)\n",
    "    # macierz gradientu wag\n",
    "    f_gradient = zeros(size(f))\n",
    "    # macierz gradientu wejścia\n",
    "    x_gradient = zeros(size(x))\n",
    "    # pętla po każdym filtrowaniu\n",
    "    for l = 1:k\n",
    "        # pętla po każdym kanałach\n",
    "        for d = 1:c\n",
    "            # pętla po każdym rzędzie obrazu wejściowego\n",
    "            for i = 1:m-p+1\n",
    "                # pętla po każdej kolumnie obrazu wejściowego\n",
    "                for j = 1:n-q+1\n",
    "                    # gradient funkcji kosztu względem pojedynczego elementu z wyjścia\n",
    "                    output_grad = g[i, j, l]\n",
    "                    # pętla po każdym elemencie w fragmencie obrazu\n",
    "                    for s = 1:p\n",
    "                        for t = 1:q\n",
    "                            # gradient funkcji kosztu względem pojedynczego elementu wagi\n",
    "                            f_gradient[s, t, d, l] += x[i+s-1, j+t-1, d] * output_grad\n",
    "                            # gradient funkcji kosztu względem pojedynczego elementu wejścia\n",
    "                            x_gradient[i+s-1, j+t-1, d] += f[s, t, d, l] * output_grad\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    tuple(x_gradient, f_gradient)\n",
    "end\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja aktywacji ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 11 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relu(x::GraphNode) = BroadcastedOperator(relu, x)\n",
    "forward(::BroadcastedOperator{typeof(relu)}, x) =return max.(0, x)\n",
    "backward(node::BroadcastedOperator{typeof(relu)}, x, g) = let\n",
    "    y = node.output\n",
    "    tuple(g .* (y .> 0))\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 12 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maxpooling(x::GraphNode) = BroadcastedOperator(maxpooling, x)\n",
    "forward(::BroadcastedOperator{typeof(maxpooling)}, x) = let\n",
    "# rozmiary danych wejściowych\n",
    "m, n, c = size(x)\n",
    "ps = 2\n",
    "qs = 2\n",
    "\n",
    "# rozmiary danych wyjściowych\n",
    "out_m, out_n = div(m, ps), div(n, qs)\n",
    "\n",
    "# wynikowa macierz maxpoolingu\n",
    "output = zeros(Float64, out_m, out_n, c)\n",
    "\n",
    "# pętla po każdym kanale\n",
    "for d = 1:c\n",
    "    # pętla po każdym rzędzie wynikowej macierzy\n",
    "    for i = 1:out_m\n",
    "        # pętla po każdej kolumnie wynikowej macierzy\n",
    "        for j = 1:out_n\n",
    "            # indeksy fragmentu macierzy wejściowej do przetworzenia\n",
    "            i_start = (i-1)*ps + 1\n",
    "            j_start = (j-1)*qs + 1\n",
    "            i_end = min(i_start+ps-1, m)\n",
    "            j_end = min(j_start+qs-1, n)\n",
    "            # fragment macierzy wejściowej do przetworzenia\n",
    "            x_fragment = x[i_start:i_end, j_start:j_end, d]\n",
    "            # maksymalna wartość w fragmencie\n",
    "            max_value, max_index = findmax(x_fragment)\n",
    "            # obliczenie pozycji elementu o najwyższej wartości\n",
    "            max_position = LinearIndices(x_fragment)[max_index]\n",
    "            # obliczenie pozycji elementu w macierzy wyjściowej\n",
    "            i_position = i_start + div(max_position-1, ps)\n",
    "            j_position = j_start + mod(max_position-1, qs)\n",
    "            # przypisanie wartości do elementu w macierzy wyjściowej\n",
    "            output[i, j, d] = max_value\n",
    "        end\n",
    "    end\n",
    "end\n",
    "output     \n",
    "end\n",
    "backward(node::BroadcastedOperator{typeof(maxpooling)}, x, g) = let\n",
    "# rozmiary danych wejściowych\n",
    "m, n, c = size(x)\n",
    "ps = 2\n",
    "qs = 2\n",
    "# rozmiary danych wyjściowych\n",
    "out_m, out_n = div(m, ps), div(n, qs)\n",
    "# macierz do przechowywania gradientów dla każdego elementu wejściowego\n",
    "grad_input = zeros(Float64, m, n, c)\n",
    "# pętla po każdym kanale\n",
    "for d = 1:c\n",
    "    # pętla po każdym rzędzie wynikowej macierzy\n",
    "    for i = 1:out_m\n",
    "        # pętla po każdej kolumnie wynikowej macierzy\n",
    "        for j = 1:out_n\n",
    "            # indeksy fragmentu macierzy wejściowej do przetworzenia\n",
    "            i_start = (i-1)*ps + 1\n",
    "            j_start = (j-1)*qs + 1\n",
    "            i_end = min(i_start+ps-1, m)\n",
    "            j_end = min(j_start+qs-1, n)\n",
    "            # fragment macierzy wejściowej do przetworzenia\n",
    "            x_fragment = x[i_start:i_end, j_start:j_end, d]\n",
    "            # maksymalna wartość w fragmencie\n",
    "            max_value, max_index = findmax(x_fragment)\n",
    "            # obliczenie pozycji elementu o najwyższej wartości\n",
    "            max_position = LinearIndices(x_fragment)[max_index]\n",
    "            # obliczenie pozycji elementu w macierzy wejściowej\n",
    "            i_position = i_start + div(max_position-1, ps)\n",
    "            j_position = j_start + mod(max_position-1, qs)\n",
    "            # przekazanie gradientu do elementu, który przyczynił się do wyznaczenia maksymalnej wartości\n",
    "            grad_input[i_position, j_position, d] += g[i, j, d]\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "grad_input\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warstwa Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 13 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flatten(x::GraphNode) = BroadcastedOperator(flatten, x)\n",
    "forward(::BroadcastedOperator{typeof(flatten)}, x) = return reshape(x, :)\n",
    "backward(node::BroadcastedOperator{typeof(flatten)}, x, g) = (reshape(g, size(x)),)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warstwa Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 14 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dense(x::GraphNode,weight::GraphNode) = BroadcastedOperator(dense, x, weight)\n",
    "forward(::BroadcastedOperator{typeof(dense)}, x, weight) = return weight * x\n",
    "backward(node::BroadcastedOperator{typeof(dense)}, x, weight, g) = let \n",
    "    # obliczenie pochodnej względem x\n",
    "    input_gradient = weight' * g\n",
    "    # obliczenie pochodnej względem wag\n",
    "    weight_gradient = g * x'\n",
    "    tuple(input_gradient, weight_gradient)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja aktywacji softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 15 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "softmax(x::GraphNode) = BroadcastedOperator(softmax, x)\n",
    "forward(::BroadcastedOperator{typeof(softmax)}, x) = return exp.(x) ./ sum(exp.(x), dims=1)\n",
    "backward(node::BroadcastedOperator{typeof(softmax)}, x, g) = let\n",
    "    y = node.output\n",
    "    #obliczenie jacobjanu\n",
    "    J = diagm(vec(y)) .- y * y'\n",
    "    #zwrócenie gradientu wejściowego\n",
    "    tuple(J' * g)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obliczanie entropi krzyżowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 16 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_entropy(y_pred::GraphNode, y_true::GraphNode) = BroadcastedOperator(cross_entropy, y_pred, y_true)\n",
    "forward(::BroadcastedOperator{typeof(cross_entropy)}, y_pred, y_true) = let\n",
    "    m = size(y_pred, 2)\n",
    "    # obliczenie wartości funkcji straty\n",
    "    loss = -1/m * sum(y_true .* log.(y_pred))\n",
    "    loss\n",
    "end\n",
    "backward(node::BroadcastedOperator{typeof(cross_entropy)}, y_pred, y_true, g) = let\n",
    "    m = size(y_pred, 2)\n",
    "    # obliczenie gradientu po y_pred\n",
    "    g_y = -1/m * (y_true ./ y_pred)\n",
    "    # zwrócenie gradientu funkcji kosztu\n",
    "    tuple(g .* g_y)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicjacja wag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_weights (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random\n",
    "function create_filter(dims::Tuple, min_val::Float64, max_val::Float64)\n",
    "    # tworzymy macierz wypełnioną zerami\n",
    "    matrix = zeros(dims)\n",
    "\n",
    "    # losujemy liczby i przypisujemy do macierzy\n",
    "    for i in 1:dims[1]\n",
    "        for j in 1:dims[2]\n",
    "            for k in 1:dims[3]\n",
    "                for l in 1:dims[4]\n",
    "                    matrix[i, j, k, l] = rand() * (max_val - min_val) + min_val\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return matrix\n",
    "end\n",
    "\n",
    "function create_weights(dims::Tuple, min_val::Float64, max_val::Float64)\n",
    "    # tworzymy macierz wypełnioną zerami\n",
    "    matrix = zeros(dims)\n",
    "\n",
    "    # losujemy liczby i przypisujemy do macierzy\n",
    "    for i in eachindex(matrix)\n",
    "        matrix[i] = rand() * (max_val - min_val) + min_val\n",
    "    end\n",
    "\n",
    "    return matrix\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicjacja pierwszej sieci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target=Variable(train_y_onehot[:,1])\n",
    "\n",
    "x = Variable((reshape(train_x[:,:,1], (28,28, 1))), name=\"input\") \n",
    "x.output = reshape(train_x[:,:,1], (28,28, 1))\n",
    "\n",
    "filters = Variable(create_filter((3,3,1,4),-0.05, 0.05))\n",
    "output1 = conv(x,filters)\n",
    "output2 = relu(output1)\n",
    "output3 = maxpooling(output2)\n",
    "output4 = flatten(output3)\n",
    "weights = Variable(create_weights((10,676),-0.05, 0.05))\n",
    "output5 = dense(output4,weights)\n",
    "output6 = softmax(output5)\n",
    "output7 = cross_entropy(output6,y_target)\n",
    "graph = topological_sort(output7)\n",
    "forward!(graph)   \n",
    "backward!(graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uczenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Accuracy: 0.6201\n",
      "Epoch: 2, Accuracy: 0.6755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Accuracy: 0.6799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Accuracy: 0.6681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Accuracy: 0.6698\n",
      "Epoch: 6, Accuracy: 0.6817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Accuracy: 0.6899\n",
      "Epoch: 8, Accuracy: 0.6993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Accuracy: 0.7036\n",
      "Epoch: 10, Accuracy: 0.7073\n"
     ]
    }
   ],
   "source": [
    "using Printf\n",
    "\n",
    "#Liczba epok\n",
    "const num_epochs = 10\n",
    "\n",
    "# Stawiamy współczynnik uczenia jako zmienną, aby łatwo go zmieniać\n",
    "const learning_rate = 0.001\n",
    "\n",
    "# Funkcja do ewaluacji modelu\n",
    "function evaluate_model(graph, test_x, test_y_onehot, test_y)\n",
    "    correct = 0\n",
    "    for i = 1:length(test_y)\n",
    "        # Przekształć dwuwymiarowe dane obrazu na trójwymiarowy tensor\n",
    "        x.output = reshape(test_x[:,:,i], (28, 28, 1))\n",
    "        # Ustaw docelowe wyjście sieci dla tego przykładu\n",
    "        y_target.output = test_y_onehot[:,i]  \n",
    "        # Oblicz wyjście sieci neuronowej\n",
    "        forward!(graph)\n",
    "        # Jeśli przewidziana cyfra zgadza się z rzeczywistą, zwiększ licznik poprawnych odpowiedzi\n",
    "        if y_target.output[argmax(output6.output)] == 1\n",
    "            correct = correct + 1 \n",
    "        end\n",
    "    end\n",
    "    return correct/length(test_y)\n",
    "end\n",
    "\n",
    "# Pętla ucząca\n",
    "for e = 1:num_epochs\n",
    "    for i = 1:length(train_y)\n",
    "        # Przekształć dwuwymiarowe dane obrazu na trójwymiarowy tensor\n",
    "        x.output = reshape(train_x[:,:,i], (28,28,1))\n",
    "        # Ustaw docelowe wyjście sieci dla tego przykładu\n",
    "        y_target.output = train_y_onehot[:,i]  \n",
    "        # Oblicz wyjście sieci neuronowej\n",
    "        forward!(graph)\n",
    "        # Jeśli wyjście sieci neuronowej jest NaN, przerwij pętlę\n",
    "        if isnan(output7.output)\n",
    "            println(\"Wartość nan na wyjściu!\")\n",
    "            break \n",
    "        end\n",
    "        # Oblicz gradienty funkcji straty i aktualizuj wagi sieci\n",
    "        backward!(graph)\n",
    "        filters.output = filters.output - filters.gradient * learning_rate\n",
    "        weights.output = weights.output - weights.gradient * learning_rate\n",
    "    end\n",
    "    \n",
    "    # Policz dokładność modelu na zbiorze testowym\n",
    "    accuracy = evaluate_model(graph, test_x, test_y_onehot, test_y)\n",
    "    # Wydrukuj dokładność modelu na zbiorze testowym\n",
    "    @printf(\"Epoch: %d, Accuracy: %.4f\\n\", e, accuracy)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
